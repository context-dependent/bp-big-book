[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blueprint Big Book of Data Analysis",
    "section": "",
    "text": "Preface\nThis is a Quarto book.\nTo learn more about Quarto books visit https://quarto.org/docs/books."
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "wf.html",
    "href": "wf.html",
    "title": "Workflow",
    "section": "",
    "text": "This section describes the standard practices that govern data analysis work at Blueprint. It covers the following: how to use git and github to maintain transparent, robust projects; how to organize data and code to make that possible; what this might look like in the near future; and what we at the Data Lab hope it will become."
  },
  {
    "objectID": "wf.html#why-workflow",
    "href": "wf.html#why-workflow",
    "title": "Workflow",
    "section": "Why Workflow?",
    "text": "Why Workflow?\nBlueprint’s data analysts have built tools and practices that suit their various needs. As a collective, we do data analysis well in the status quo model. Why establish standards for organizing and executing data analysis projects? Because we think that the whole process of data analysis could be easier to do, easier to learn, more predictable for managers, more collaborative for data goblins, and make more contributions to our collective intelligence as a community of practice."
  },
  {
    "objectID": "wf.html#recommended-reading",
    "href": "wf.html#recommended-reading",
    "title": "Workflow",
    "section": "Recommended Reading",
    "text": "Recommended Reading\nThroughout this section, we make reference to a number of resources created by the R development and data analysis community. Each is a worthwhile read in its own right, and we recommend that you check them out at some point.\n\nBryan, J. Happy Git and GitHub for the useR\nThe ur-text for R users looking to integrate git and Github into their analysis workflows. Bryan keeps it fun and friendly while providing a thorough introduction to the nuts and bolts of getting started. Closer to required than recommended reading.\n\n\nWickham, H. R for Data Science\nHadley is the GOAT R developer, and this book – while a bit sparse and introductory – offers valuable advice and instruction across the whole process, with some starting points for analysis workflow.\n\n\nGit Commands Cheatsheet\nGit can do many different things, and so comes with many different commands. This is a fairly comprehensive reference. Use it if you know what you want to do, you know git can do it, but you’re just not sure which command will make it happen."
  },
  {
    "objectID": "wf-git.html",
    "href": "wf-git.html",
    "title": "2  Git and Github",
    "section": "",
    "text": "Git is a version control system. It tracks the changes you make and commit to files in a local repository. It lets you apply those changes to remote repositories hosted on services like Github."
  },
  {
    "objectID": "wf-git.html#gitting-rolling",
    "href": "wf-git.html#gitting-rolling",
    "title": "2  Git and Github",
    "section": "2.2 Gitting rolling",
    "text": "2.2 Gitting rolling\nAt a minimum, you’ll need to install git and register a Github account. Optionally, you can also integrate git and Github into your RStudio environment. If you’ve done those things already, great, you’re good to go! Otherwise, it is strongly recommended that you pause what you’re doing now and read / work through Happy Git before continuing."
  },
  {
    "objectID": "wf-git.html#usage-at-blueprint-as-of-may-2023",
    "href": "wf-git.html#usage-at-blueprint-as-of-may-2023",
    "title": "2  Git and Github",
    "section": "2.3 Usage at Blueprint (as of May 2023)",
    "text": "2.3 Usage at Blueprint (as of May 2023)\nRight now, usage is highly varied. Some members of the Data Lab team use git and Github for version control and hosting of both R packages and analysis project code. It is not, however, part of the typical analysis project. We will begin rolling out the basic workflow in the summer of 2023 and see how it goes."
  },
  {
    "objectID": "wf-structure.html",
    "href": "wf-structure.html",
    "title": "3  Folder Structure",
    "section": "",
    "text": "Currently, the typical analysis project puts code and data in the same folder. This has the benefit of increasing the technical portability of the analysis project. The analysis folder comes with ‘batteries included’, meaning that it could hypothetically be run immediately after being copied or moved to a different location. This technical benefit is, however, outweighed by its costs.\nFirst, because the data can’t leave the Z drive, the fact that you could move them doesn’t mean that you should. Second, the structural coupling of data and code has also constrained our ability to institute meaningful version control, since the data can’t be versioned so the project repos can’t actually run. Finally, the same data are sometimes required by multiple analysis projects, which has led to either not-strictly necessary duplication of data, or truly byzantine and fragile flows of information."
  },
  {
    "objectID": "wf-structure.html#a-conceptual-skeleton",
    "href": "wf-structure.html#a-conceptual-skeleton",
    "title": "3  Folder Structure",
    "section": "3.1 A Conceptual Skeleton",
    "text": "3.1 A Conceptual Skeleton\nThe diagram below presents a basic revision of the aforementioned structure. The two substantial differences are:\n\nrather than living within the same folder, data and code live in different places. Data stay in the project’s Z drive folder; code, outputs, figures in the analysis repo (folder).\nthe project’s analysis repo is versioned with git and reflected in a remote repo hosted on GitHub\n\n\n\n\n\nflowchart LR\n    P(Project Library)\n    MP(My Project)\n    D[Intranet folder including \\nDeliverables, Admin docs, etc.]\n    P --- MP\n    MP --- D\n    Z(Z-drive)\n    MZ(My Project)\n    S[Encrypted data folder including\\nSurvey Exports, Program Data, etc.]\n    Z --- MZ\n    MZ --- S\n    L(my-project)\n    ML(My Laptop)\n    C[local git repository, containing\\nCode, reports, figures]\n    ML --- L\n    L --- C\n    GH(Github)\n    R(my-username/my-project)\n    CR[remote git repository\\nreflecting its local counterpart]\n    GH --- R\n    R --- CR\n\n\n\n\n\n\n\n\nThis concept intentionally leaves the substructure of the data folder Z:/My Project and the analysis repo My Laptop/my-project up to the judgement and preference of the project’s specific team. The key difference from the typical current structure is strictly that data (and only data) live in the Z-drive.\n\n\n\n\n\n\nGuidance and tools for setting up analysis repos according to your needs is forthcoming, but it will be in a different section."
  },
  {
    "objectID": "wf-structure.html#what-else-is-new",
    "href": "wf-structure.html#what-else-is-new",
    "title": "3  Folder Structure",
    "section": "3.2 What else is new?",
    "text": "3.2 What else is new?\nLocating analysis code and outputs outside of the Z-drive entails some considerations and requirements that were previously irrelevant:\n\nRemote analysis repositories (GitHub) should be set to private before any outputs are generated. Any colleagues who need to see the code should be invited to collaborate.\nPaths to data files used in code will need to be “hard-coded”, meaning that they will need to begin with “Z:/My Project/”. We’re working on ways to make this easier and more flexible, but hard-coding (in this case) will definitely work.\nBecause files and folders in Z:/My Project will be referred to directly, the names of and paths to files and folders in Z:/My Project should, unless absolutely necessary, be left unchanged from the time that they are created.\nEnsure that outputs (reports, figures, tables) don’t include any personal identifying information. While they won’t be available to the general public in a private Github repo, our policy is to confine such sensitive information to our Z-drive."
  },
  {
    "objectID": "wf-basic.html",
    "href": "wf-basic.html",
    "title": "4  Basic Workflow",
    "section": "",
    "text": "So you’ve got your project library folder, Z-drive data folder, and analysis repository set up. What now? In this brief chapter, we’ll walk through the loop that constitutes the basic workflow, along with some guidance on new situations that might come up."
  },
  {
    "objectID": "wf-basic.html#creating-an-analysis-repo",
    "href": "wf-basic.html#creating-an-analysis-repo",
    "title": "4  Basic Workflow",
    "section": "4.1 Creating an Analysis Repo",
    "text": "4.1 Creating an Analysis Repo\nAn analysis repo is basically just an R project. You can set one up through the RStudio GUI, or with the usethis package. usethis also includes handy helpers for initializing a repository, so I tend to rely on it for each step.\nusethis::create_project(\"my-project\")\nThen, open the project folder in your development environment (RStudio or VSCode), and run the following in your R console1.\nusethis::use_git()\nusethis::use_github(private = TRUE)\nAssuming you have set up your Github credentials, your analysis repo will be up and running locally and remotely."
  },
  {
    "objectID": "wf-basic.html#narrowing-the-scope",
    "href": "wf-basic.html#narrowing-the-scope",
    "title": "4  Basic Workflow",
    "section": "4.2 Narrowing the scope",
    "text": "4.2 Narrowing the scope\nAs mentioned previously, git can do a lot. For this basic workflow, you’re only going to use three commands to make it run: git add git commit and git push. We assume you’re the only one working on the analysis repo. If you are collaborating with others, and especially if you are working on different aspects of the project at the same time, please reach out for more guidance if you’re feeling unsure."
  },
  {
    "objectID": "wf-basic.html#the-basic-pattern",
    "href": "wf-basic.html#the-basic-pattern",
    "title": "4  Basic Workflow",
    "section": "4.3 The Basic Pattern",
    "text": "4.3 The Basic Pattern\n\nIdentify a task (like “link survey data”, “clean program records”, “explore demographics”)\nWrite the code that completes the task (you’ve been here before)\ngit add -A to stage your changes to the analysis repo\ngit commit -m \"link survey data\" 2 to commit your changes with the message “link survey data” (use whatever task you’re actually doing)\ngit push -u origin main to upload your local changes to the remote (github) repository.\n\nThis basic pattern is a simplified version of the Repeated Amend workflow pattern described in Happy Git. You’re welcome to use that pattern as well, or explore Branching and Merging."
  },
  {
    "objectID": "wf-basic.html#what-can-i-do-now-that-i-couldnt-before",
    "href": "wf-basic.html#what-can-i-do-now-that-i-couldnt-before",
    "title": "4  Basic Workflow",
    "section": "4.4 What can I do now that I couldn’t before?",
    "text": "4.4 What can I do now that I couldn’t before?\n\nAsk for help: if you’ve run into a problem that you don’t feel equipped to solve, you can ask one of your colleagues (with access to the Z-drive data folder) to clone the analysis repo, run your code, and offer edits, advice, or whatever else would help.\nShow your work: the analysis repo is less sensitive than the data folder, and so can be shown to any Blueprinter, whether they are on the project team or not.\nFind solutions: get inspiration from other data analyzing printers using the same approach by borrowing their code."
  },
  {
    "objectID": "wf-goal.html",
    "href": "wf-goal.html",
    "title": "6  Advanced / Aspirational Workflow",
    "section": "",
    "text": "This section is still quite rough. If you’re here, you’re welcome to drop suggested changes, additions, or clarifications in the comments. Your input will be incorporated into the development process.\n\n\n\nBolded items are clear hooks for tools, rules, standards.\nAP[X] is the Xth analysis planning stage R[X] is the Xth artefact of the analysis process\n\nPre-contract\n\n\nProposal\nResearch objectives\nAP0: Preliminary plan\n\nR0: High-level methodology\n\n\n\nData design\n\n\nSurvey designs\nAdmin datasets\nAP1: Specify approach\n\nR1: Detailed methodology\nR2: Input data dictionary\n\n\n\nData collection\n\n\nMonitoring:\n\nR3: Monitoring log\n\n\n\nAnalyze\n\n\nHave the data for the analysis\nHave accumulated insights\nHave monitoring logs\nExplore loop:\n\nR4: Exploratory output pile\nR5: Decision log\n\nAP2: revisit R1: Detailed methodology\n\nRealign analysis objectives with project objectives\nAssess feasibility through exploration\nIdentify limitations and enumerate implications\nJustify methodological adapatations\nIteratively build R6: data pipeline\nDocument final data pipeline\nFinalize methodology\n\n\n\nPublish\n\n\nAP3: Schedule of deliverable outputs\n\nR7: Analysis outputs"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Knuth, Donald E. 1984. “Literate Programming.” Comput.\nJ. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "wf-getting-help.html#general-approach",
    "href": "wf-getting-help.html#general-approach",
    "title": "5  Getting Help",
    "section": "5.1 General Approach",
    "text": "5.1 General Approach\nThe Data Lab team is here to help you succeed in your work. Each of us is a subject matter expert in one or more areas of data analysis, and we’re happy to share our knowledge with you, whether you’re a new analyst or a seasoned pro. However, we’re also a small team, and we have a lot of work to do. This process is designed to help us help you (and future analysts with similar questions) find solutions as efficiently as possible. If you follow it, you’ll get the help you need faster, and you’ll be a part of building our collective knowledge base.\n\n\n\n\n\n\n\nD\n\n  \n\nconsult\n\n Consult available resources   \n\nif_exists\n\n Does a solution already exist?   \n\nconsult->if_exists\n\n    \n\nyes_exists\n\n Use existing solution   \n\nif_exists->yes_exists\n\n  Yes   \n\nno_exists\n\n Request help   \n\nif_exists->no_exists\n\n  No   \n\ndesc\n\n Describe the problem   \n\nno_exists->desc\n\n    \n\nclarify\n\n Clarify the problem   \n\ndesc->clarify\n\n    \n\ncollab\n\n Collaborate on a solution   \n\nclarify->collab\n\n    \n\ndocument\n\n Document the solution for future reference   \n\ncollab->document\n\n   \n\n\n\n\n\n\n5.1.1 Consult Available Resources\nBefore you ask for help, take a moment to consider whether a solution already exists. This website is still under construction, but it will eventually contain a wealth of information about the tools and processes we use. If you’re having trouble with a particular tool, check the documentation for that tool. If you are looking for a statistical method to answer a particular question or achieve a specific result, find reference papers that answer similar questions with similar data. If you find a solution that fits your use case, great! you’re done, but even if you don’t, the information you gather will help you describe your problem to the Data Lab team, and point us very specifically in the right direction to help you.\nWhich resources should you check before asking for help? Here are some options:\n\nChatGPT: This is the first place you should go if you run into a coding issue or are unsure how to accomplish a particular task. ChatGPT is proficient in writing R code, so you can describe your coding issue or the task you want to accomplish, and it can provide you with the code or guide you on how to approach the problem yourself. It often makes small coding errors for more complex tasks, but if you show it the error then it can often fix the code it wrote previously.\nStack Overflow: This is a popular question-and-answer website for programming-related queries, including R. It has a vast community of developers who actively participate in discussions and provide solutions to various coding problems. You can search for R-related questions using specific tags or keywords and explore existing answers or ask new questions if you couldn’t find a suitable solution. A Google search for ‘Stack Overflow [your question here]’ will usually turn up lots of helpful discussions, unless the question is very niche.\nR Documentation: The official R documentation can be a bit technical, but it is a comprehensive resource that provides information about R packages, functions, and their usage. You can access it online or directly from the R console using the help() or ? command followed by the package or function name. For example, running the R command help(dplyr) or ?dplyr will cause information about the dplyr package to pop up in your RStudio window.\nCRAN (Comprehensive R Archive Network): CRAN is a network of servers that host R packages. It is the place R packages live, along with detailed PDF files documenting every function in a package, and examples (sometimes called ‘vignettes’) written by the package creator to demonstrate common use-cases for the functions in a package. You can visit the CRAN website (https://cran.r-project.org/) to search for packages related to your specific needs, or a Google search for something like ‘[this package] vignettes’ or ‘[this package] documentation PDF’ will turn up what you’re looking for.\n\nThese resources can enhance your understanding of R, improve your problem-solving skills, and empower you to overcome coding challenges on your own. But they can take some time to get used to. If these resources don’t seem to be giving you what you need, or if you’d just prefer to talk over the problem with a real person, don’t hesitate to reach out to the Data Lab team for support.\n\n\n5.1.2 Request Help\nIf you can’t find a solution to your problem, it’s time to ask for help. Request help by emailing / slacking one of the Data Lab support team members directly, or by posting a message in the #data-help channel on Slack.\n\n\n\n\n\n\nData Lab support team\n\n\n\n\nThomas McManus\nAlex Rand\n\n\n\n\n\n5.1.3 Describe the Problem\nA problem description will be most useful if it answers these questions:\n\nWhat are you trying to do?\nWhat have you tried so far?\nWhat happened when you tried it?\nWhat do you think should have happened instead?\nWhat is your best guess about the shape, nature, or source of the solution?\n\nThe more specific you can be, the better, but don’t worry if you don’t know the answers to all of these questions. Getting help is a collaborative process, and we’ll work with you to fill in the gaps in your knowledge.\n\n\n\n\n\n\nReproducible Examples (Reprexes)\n\n\n\nFor problems that involve code, the best way to describe your problem is to provide a reproducible example, or reprex. A reprex is a minimal, self-contained example that demonstrates the problem you’re having. It should be as simple as possible, but no simpler. It should run on any machine with the right software, and it should produce the same error or unexpected result that you’re seeing in your own work.\nFor more information on how to create a reprex, see the Reprex chapter of the Tidyverse Reprex Guide.\n\n\n\n\n5.1.4 Clarify the Problem\nOnce you’ve described your problem, we’ll partner you with a Data Lab team member who has the right expertise to help you. They’ll work with you to ensure they understand your problem and the context in which it arose. If they need more information, they’ll ask you for it. If they need to see your code, they’ll ask you to share it. If more help is needed, they’ll bring in other team members with the right expertise.\n\n\n5.1.5 Collaborate on a Solution\nOnce we understand your problem, we’ll work with you to find a solution. This will be a collaborative, iterative process – we’ll likely need to go back-and-forth with you a few times before we find the answer we need. It will look something like this:\n\nThink of a possible solution: A Data Lab team member will propose a solution or a few possible solutions to address the problem. The Data Lab team member will explain the rationale behind each approach, and together you’ll decide how best to try them out. You should feel free to ask lots of questions at this stage.\nExperiment and get feedback: Together with the Data Lab team member, you can experiment with the proposed solutions, implementing changes to the code as necessary. It’s important to communicate the outcome of each thing you try, sharing any error messages, unexpected outputs, or progress made. This iterative feedback loop helps to narrow down potential solutions and uncover any unforeseen challenges.\n\n\n\n5.1.6 Document the Solution for Future Reference\nUnless your problem is highly specific to your project, we’ll ask for your help to document the solution for future reference. Most likely this will be in the form of a .qmd Quarto document to be integrated into this book, but this may vary on a case-by-case basis.\nTo write good documentation for your solution you can follow these steps:\n\nCapture the context: Start by providing a brief overview of the problem you were trying to solve. Describe the specific scenario, project, or task that led to the issue. Include any relevant background information, data sources, or dependencies that influenced the solution.\nExplain the approach that worked: explain in general step-by-step terms how you solved the problem. approach or strategy used to address the problem.\nInclude code snippets: It will be helpful to future analysts to see the code you used to solve your problem. Highlight the specific changes made and provide comments within the code snippets to clarify their purpose and functionality.\nProvide examples of the successful result: If applicable, include example outputs or results to demonstrate the successful resolution of the problem. This could include visualizations, summary statistics, or anything else the code creates.\nAddress potential pitfalls: Is your solution custom-tailored to your particular dataset in some way where it may not help in most other situations? Are you aware of any specific situations where your solution might not work? Identify and address potential pitfalls or limitations of the solution. Document things you think could impact the solution’s applicability in different scenarios. This helps future analysts anticipate potential challenges.\nInclude references: If you used specific resources, tutorials, documentation, or Stack Overflow posts during the troubleshooting process, provide links to those sources. This helps ensure that even if your specific code doesn’t help in some other specific situation, the future analyst will know where to look for help applying the general approach to their situation."
  },
  {
    "objectID": "wf-git.html#why-git-and-github",
    "href": "wf-git.html#why-git-and-github",
    "title": "2  Git and Github",
    "section": "2.1 Why Git and Github?",
    "text": "2.1 Why Git and Github?\nIn 2017, Jennifer Bryan expanded on the ‘Why Git?’ chapter of her invaluable instructional book in the paper Excuse me, do you have a moment to talk about version control?. She articulates the benefits so clearly that they are not worth rephrasing:\n\nDoing your work becomes tightly integrated with organizing, recording, and disseminating it. It’s not a separate, burdensome task you are tempted to neglect.\nCollaboration is much more structured, with powerful tools for asynchronous work and managing versions.\nThe marginal effort required to create a web presence for a project is negligible.\nBy using common mechanics across work modes (research, teaching, analysis), you achieve basic competence quickly and avoid the demoralizing forget-relearn cycle.\n\nIf you have worked on a data analysis project at Blueprint, particularly one that involves collaboration, you have undoubtedly done the following:\n\nSearching for an explanation about when or why a specific choice about processing the data was made\nExperienced disorientation attempting to navigate someone else’s code\nSpent hours undoing a change that breaks the pipeline\nWanted someone’s help or advice about how to solve a problem, but didn’t feel like it would be worth getting them access\n\nHosted version control like Github empowers you with tools to simplify and routinize any of these situations."
  },
  {
    "objectID": "wf-pilots.html#using-github-issues-to-host-a-projects-decision-log",
    "href": "wf-pilots.html#using-github-issues-to-host-a-projects-decision-log",
    "title": "7  Workflow Pilots",
    "section": "7.1 Using Github Issues to Host a Project’s Decision Log",
    "text": "7.1 Using Github Issues to Host a Project’s Decision Log\n\n\n\nMeta\ndata\n\n\n\n\nproject\nFSC Benchmarks\n\n\nrepo\nalex-rand/fsc-benchmarks\n\n\ncontact\n(alex-rand?)\n\n\n\n\n7.1.1 What is the practice?\n\nuse Github’s Issues tracker to:\n\nask and answer questions about program, data, and methods\ncollaboratively arrive at decisions about the best path forward\nlabel conversations to help others focus their contributions\n\nconnect tasks (branches, pull requests) to the conversations that produced them\n\n\n\n7.1.2 Why is it good?\n\nlives in an obvious, highly accessible place attached to the project’s repo\nenables transparent, asynchronous, collaborative decision-making\norganically documents that process\nprovides an intuitive interface for tracking down the rationale for decisions which would otherwise be forgotten\nlabels make it easy to highlight issues for specific team members or cross-functional collaborators\ncode reviewers have a clear first stop for answering questions about decisions reflected in the code they’re reviewing\n\n\n\n7.1.3 What does it look like?\n\n7.1.3.1 Browsing open issues\n\n\n\nA List of Github Issues\n\n\n\n\n7.1.3.2 Filtering for the ones you might be able to help with\n\n\n\nFiltered list of Gihub Issues\n\n\n\n\n7.1.3.3 An active discussion\n\n\n\nDiscussing issue #30\n\n\n\n\n7.1.3.4 Making a branch for work associated with an issue\n\n\n\nCreating a branch connected to the issue\n\n\n\n\n7.1.3.5 Searching past issues\n\n\n\nSearching through closed issues"
  },
  {
    "objectID": "viz.html#recomended-reading",
    "href": "viz.html#recomended-reading",
    "title": "Visualization",
    "section": "Recomended Reading",
    "text": "Recomended Reading\nMost of the recommended readings in this section focus on the conceptual approach to tackling visualization problems. This is intentional, since designing the right visualization is by far the deeper and more complex challenge than instructing a computer how to make it.\n\nAndrew Heiss, Data Visualization\nA thorough yet accessible free course on data visualization in general, illustrated with practical applications in R and ggplot.\n\n\nEdward Tufte, The Visual Display of Quantitative Information\nThe way that information is visualized in academia, business, and the media makes Edward Tufte sick. In this book, he explains what makes him mad, why it infuriates him so, and how it could be different. While his puritanical minimalism often overshoots its target (occasionally at the expense of clarity), it will transform the way you think about designing charts, tables, and maps for the better.\n\n\nAlberto Cairo, The Functional Art\nCairo comes from a similar philosophical perspective as Tufte, but is in turns more pragmatic, patient, and permissive. Instead of characterizing any non-informative or redundant element of a visualization as ‘chart-junk’, he develops a useful concept of the different objectives visualizations can be designed to achieve, and discussses the tradeoff between depth and familiarity with a good deal more nuance.\n\n\nHadley Wickham et al, ggplot2\nAn in-depth guide to ggplot2, the standard software package for visualization at Blueprint."
  },
  {
    "objectID": "viz-why.html#exploration",
    "href": "viz-why.html#exploration",
    "title": "8  Why Visualize?",
    "section": "8.1 Exploration",
    "text": "8.1 Exploration\n\ntesting your prior expectations about the data\nidentifying patterns and relationships\nvisually confirming the results of processing"
  },
  {
    "objectID": "viz-why.html#presentation",
    "href": "viz-why.html#presentation",
    "title": "8  Why Visualize?",
    "section": "8.2 Presentation",
    "text": "8.2 Presentation\n\nsupporting an argument\nanchoring an explanation\nimpactfully illustrating a comparison"
  },
  {
    "objectID": "viz-how.html#for-exploration",
    "href": "viz-how.html#for-exploration",
    "title": "9  How to Visualize",
    "section": "9.1 For exploration",
    "text": "9.1 For exploration"
  },
  {
    "objectID": "viz-how.html#for-presentation",
    "href": "viz-how.html#for-presentation",
    "title": "9  How to Visualize",
    "section": "9.2 For presentation",
    "text": "9.2 For presentation"
  },
  {
    "objectID": "viz-how.html#ggplot2",
    "href": "viz-how.html#ggplot2",
    "title": "9  How to Visualize",
    "section": "9.3 ggplot2",
    "text": "9.3 ggplot2"
  },
  {
    "objectID": "viz-how.html#other-software",
    "href": "viz-how.html#other-software",
    "title": "9  How to Visualize",
    "section": "9.4 Other software",
    "text": "9.4 Other software"
  },
  {
    "objectID": "sd.html",
    "href": "sd.html",
    "title": "Developing Your Skills",
    "section": "",
    "text": "This chapter is currently under active development. Use its content as it is helpful, but please moderate your expectations. If you’re interested in helping out with its development, head over to the repo\n\n\n\nThe Blueprint Data Lab’s mandate is not merely to ensure that our firm remains on the cutting edge of data-driven solutions, but also to foster an environment of growth, learning, and curiosity. The world of data is continuously evolving, with new techniques, technologies, and paradigms emerging regularly. As such, continuous learning across the organization is not only beneficial but critical for us to remain adept and agile in navigating the complex challenges we find ourselves faced with every day.\nTo thrive in our work, we must harness the , using it to shape insights and drive decision-making. Developing our skills in four main areas—programming, statistics, analysis, and visualization—is thus integral to our work as consultants. These are the pillars of data literacy that form the foundation of our professional expertise.\n\nProgramming: In an age where automation is king, knowing how to code is essential. Programming allows us to extract, clean, process, and manipulate data with precision and speed. It opens up vast possibilities for data exploration and analysis that manual methods simply cannot match.\nStatistics: This is the language of data. It provides the mathematical framework for understanding, interpreting, and making inferences from data. A solid grounding in statistics is necessary for us to distill sense from the noisy world of raw data and to validate the insights we generate.\nAnalysis: This skill involves putting the tools of programming and the principles of statistics to work together. It’s about identifying patterns and trends, solving problems, and making meaning. It’s an investigative process that demands both rigorous scientific thinking and creative intuition.\nVisualization: It’s one thing to find insights in data; it’s another to communicate them effectively. Data visualization is about using graphical representations to tell the story of the data in a way that is intuitive and compelling. It bridges the gap between data experts and non-experts, making our insights accessible to all.\nEquity: In alignment with Blueprint’s core values, we must do our best to ensure that our data analysis forwards the goals of a more equitable and just society. This means more than just identifying and mitigating the analytical risks posed by structural, societal, and methodological biases. It means situating any data analysis we carry out within an anti-oppression (anti-racist, anti-colonialist, anti-ablist) philosophical framework.\n\nIn this chapter, “Developing Your Skills,” you’ll find a curated list of resources designed to help you strengthen these four pillars. They range from introductory courses for those new to the field, to advanced readings for seasoned professionals seeking to deepen their knowledge. Each resource has been carefully selected based on its educational value, practical relevance, and alignment with our firm’s work and values.\nRemember, as professionals in a field that’s ever-changing, our learning journey never ends. It’s my hope that this resource guide will inspire you to continuously cultivate your skills, innovate in your work, and strive towards mastery. With the right tools, the right mindset, and a dedication to lifelong learning, there’s no limit to the impact we can achieve.\nWelcome to this exciting chapter of your professional development."
  },
  {
    "objectID": "sd-courses.html",
    "href": "sd-courses.html",
    "title": "11  Courses",
    "section": "",
    "text": "Programming Languages\nNand2Tetris\nAlgorithms\nStatistical Rethinking\nBayesian Statistics\nComputational Methods for Social Science Data\nMicroeconomics for Public Policy"
  }
]
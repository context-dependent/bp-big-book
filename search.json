[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blueprint Big Book of Data Analysis",
    "section": "",
    "text": "This is a Quarto book.\nTo learn more about Quarto books visit https://quarto.org/docs/books."
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "See Knuth (1984) for additional discussion of literate programming.\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "wf.html",
    "href": "wf.html",
    "title": "Workflow",
    "section": "",
    "text": "This section describes the standard practices that govern data analysis work at Blueprint. It covers the following: how to use git and github to maintain transparent, robust projects; how to organize data and code to make that possible; what this might look like in the near future; and what we at the Data Lab hope it will become."
  },
  {
    "objectID": "wf.html#why-workflow",
    "href": "wf.html#why-workflow",
    "title": "Workflow",
    "section": "Why Workflow?",
    "text": "Why Workflow?\nBlueprint’s data analysts have built tools and practices that suit their various needs. As a collective, we do data analysis well in the status quo model. Why establish standards for organizing and executing data analysis projects? Because we think that the whole process of data analysis could be easier to do, easier to learn, more predictable for managers, more collaborative for data goblins, and make more contributions to our collective intelligence as a community of practice."
  },
  {
    "objectID": "wf.html#recommended-reading",
    "href": "wf.html#recommended-reading",
    "title": "Workflow",
    "section": "Recommended Reading",
    "text": "Recommended Reading\nThroughout this section, we make reference to a number of resources created by the R development and data analysis community. Each is a worthwhile read in its own right, and we recommend that you check them out at some point.\n\nBryan, J. Happy Git and GitHub for the useR\nThe ur-text for R users looking to integrate git and Github into their analysis workflows. Bryan keeps it fun and friendly while providing a thorough introduction to the nuts and bolts of getting started. Closer to required than recommended reading.\n\n\nWickham, H. R for Data Science\nHadley is the GOAT R developer, and this book – while a bit sparse and introductory – offers valuable advice and instruction across the whole process, with some starting points for analysis workflow."
  },
  {
    "objectID": "wf-git.html",
    "href": "wf-git.html",
    "title": "2  Git and Github",
    "section": "",
    "text": "Git is a version control system. It tracks the changes you make and commit to files in a local repository. It lets you apply those changes to remote repositories hosted on services like Github."
  },
  {
    "objectID": "wf-git.html#gitting-rolling",
    "href": "wf-git.html#gitting-rolling",
    "title": "2  Git and Github",
    "section": "2.1 Gitting rolling",
    "text": "2.1 Gitting rolling\nAt a minimum, you’ll need to install git and register a Github account. Optionally, you can also integrate git and Github into your RStudio environment. If you’ve done those things already, great, you’re good to go! Otherwise, it is strongly recommended that you pause what you’re doing now and read / work through Happy Git before continuing."
  },
  {
    "objectID": "wf-git.html#usage-at-blueprint-as-of-may-2023",
    "href": "wf-git.html#usage-at-blueprint-as-of-may-2023",
    "title": "2  Git and Github",
    "section": "2.2 Usage at Blueprint (as of May 2023)",
    "text": "2.2 Usage at Blueprint (as of May 2023)\nRight now, usage is highly varied. Some members of the Data Lab team use git and Github for version control and hosting of both R packages and analysis project code. It is not, however, part of the typical analysis project. We will begin rolling out the basic workflow in the summer of 2023 and see how it goes."
  },
  {
    "objectID": "wf-structure.html",
    "href": "wf-structure.html",
    "title": "3  Folder Structure",
    "section": "",
    "text": "Currently, the typical analysis project puts code and data in the same folder. This has the benefit of increasing the technical portability of the analysis project. The analysis folder comes with ‘batteries included’, meaning that it could hypothetically be run immediately after being copied or moved to a different location. This technical benefit is, however, outweighed by its costs.\nFirst, because the data can’t leave the Z drive, the fact that you could move them doesn’t mean that you should. Second, the structural coupling of data and code has also constrained our ability to institute meaningful version control, since the data can’t be versioned so the project repos can’t actually run. Finally, the same data are sometimes required by multiple analysis projects, which has led to either not-strictly necessary duplication of data, or truly byzantine and fragile flows of information."
  },
  {
    "objectID": "wf-structure.html#a-conceptual-skeleton",
    "href": "wf-structure.html#a-conceptual-skeleton",
    "title": "3  Folder Structure",
    "section": "3.1 A Conceptual Skeleton",
    "text": "3.1 A Conceptual Skeleton\nThe diagram below presents a basic revision of the aforementioned structure. The two substantial differences are:\n\nrather than living within the same folder, data and code live in different places. Data stay in the project’s Z drive folder; code, outputs, figures in the analysis repo (folder).\nthe project’s analysis repo is versioned with git and reflected in a remote repo hosted on GitHub\n\n\n\n\n\nflowchart LR\n    P(Project Library)\n    MP(My Project)\n    D[Intranet folder including \\nDeliverables, Admin docs, etc.]\n    P --- MP\n    MP --- D\n    Z(Z-drive)\n    MZ(My Project)\n    S[Encrypted data folder including\\nSurvey Exports, Program Data, etc.]\n    Z --- MZ\n    MZ --- S\n    L(my-project)\n    ML(My Laptop)\n    C[local git repository, containing\\nCode, reports, figures]\n    ML --- L\n    L --- C\n    GH(Github)\n    R(my-username/my-project)\n    CR[remote git repository\\nreflecting its local counterpart]\n    GH --- R\n    R --- CR\n\n\n\n\n\n\n\n\nThis concept intentionally leaves the substructure of the data folder Z:/My Project and the analysis repo My Laptop/my-project up to the judgement and preference of the project’s specific team. The key difference from the typical current structure is strictly that data (and only data) live in the Z-drive.\n\n\n\n\n\n\nGuidance and tools for setting up analysis repos according to your needs is forthcoming, but it will be in a different section."
  },
  {
    "objectID": "wf-structure.html#what-else-is-new",
    "href": "wf-structure.html#what-else-is-new",
    "title": "3  Folder Structure",
    "section": "3.2 What else is new?",
    "text": "3.2 What else is new?\nLocating analysis code and outputs outside of the Z-drive entails some considerations and requirements that were previously irrelevant:\n\nRemote analysis repositories (GitHub) should be set to private before any outputs are generated. Any colleagues who need to see the code should be invited to collaborate.\nPaths to data files used in code will need to be “hard-coded”, meaning that they will need to begin with “Z:/My Project/”. We’re working on ways to make this easier and more flexible, but hard-coding (in this case) will definitely work.\nBecause files and folders in Z:/My Project will be referred to directly, the names of and paths to files and folders in Z:/My Project should, unless absolutely necessary, be left unchanged from the time that they are created.\nEnsure that outputs (reports, figures, tables) don’t include any personal identifying information. While they won’t be available to the general public in a private Github repo, our policy is to confine such sensitive information to our Z-drive."
  },
  {
    "objectID": "wf-basic.html",
    "href": "wf-basic.html",
    "title": "4  Basic Workflow",
    "section": "",
    "text": "So you’ve got your project library folder, Z-drive data folder, and analysis repository set up. What now? In this brief chapter, we’ll walk through the loop that constitutes the basic workflow, along with some guidance on new situations that might come up."
  },
  {
    "objectID": "wf-basic.html#creating-an-analysis-repo",
    "href": "wf-basic.html#creating-an-analysis-repo",
    "title": "4  Basic Workflow",
    "section": "4.1 Creating an Analysis Repo",
    "text": "4.1 Creating an Analysis Repo\nAn analysis repo is basically just an R project. You can set one up through the RStudio GUI, or with the usethis package. usethis also includes handy helpers for initializing a repository, so I tend to rely on it for each step.\nusethis::create_project(\"my-project\")\nThen, open the project folder in your development environment (RStudio or VSCode), and run the following in your R console1.\nusethis::use_git()\nusethis::use_github(private = TRUE)\nAssuming you have set up your Github credentials, your analysis repo will be up and running locally and remotely."
  },
  {
    "objectID": "wf-basic.html#narrowing-the-scope",
    "href": "wf-basic.html#narrowing-the-scope",
    "title": "4  Basic Workflow",
    "section": "4.2 Narrowing the scope",
    "text": "4.2 Narrowing the scope\nAs mentioned previously, git can do a lot. For this basic workflow, you’re only going to use three commands to make it run: git add git commit and git push. We assume you’re the only one working on the analysis repo. If you are collaborating with others, and especially if you are working on different aspects of the project at the same time, please reach out for more guidance if you’re feeling unsure."
  },
  {
    "objectID": "wf-basic.html#the-basic-pattern",
    "href": "wf-basic.html#the-basic-pattern",
    "title": "4  Basic Workflow",
    "section": "4.3 The Basic Pattern",
    "text": "4.3 The Basic Pattern\n\nIdentify a task (like “link survey data”, “clean program records”, “explore demographics”)\nWrite the code that completes the task (you’ve been here before)\ngit add -A to stage your changes to the analysis repo\ngit commit -m \"link survey data\" 2 to commit your changes with the message “link survey data” (use whatever task you’re actually doing)\ngit push -u origin main to upload your local changes to the remote (github) repository.\n\nThis basic pattern is a simplified version of the Repeated Amend workflow pattern described in Happy Git. You’re welcome to use that pattern as well, or explore Branching and Merging."
  },
  {
    "objectID": "wf-basic.html#what-can-i-do-now-that-i-couldnt-before",
    "href": "wf-basic.html#what-can-i-do-now-that-i-couldnt-before",
    "title": "4  Basic Workflow",
    "section": "4.4 What can I do now that I couldn’t before?",
    "text": "4.4 What can I do now that I couldn’t before?\n\nAsk for help: if you’ve run into a problem that you don’t feel equipped to solve, you can ask one of your colleagues (with access to the Z-drive data folder) to clone the analysis repo, run your code, and offer edits, advice, or whatever else would help.\nShow your work: the analysis repo is less sensitive than the data folder, and so can be shown to any Blueprinter, whether they are on the project team or not.\nFind solutions: get inspiration from other data analyzing printers using the same approach by borrowing their code."
  },
  {
    "objectID": "wf-goal.html",
    "href": "wf-goal.html",
    "title": "5  Advanced / Aspirational Workflow",
    "section": "",
    "text": "TODO"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Knuth, Donald E. 1984. “Literate Programming.” Comput.\nJ. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  }
]
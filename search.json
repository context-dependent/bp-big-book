[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blueprint Big Book of Data Analysis",
    "section": "",
    "text": "Preface\nThis is a Quarto book.\nTo learn more about Quarto books visit https://quarto.org/docs/books."
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "wf.html#why-workflow",
    "href": "wf.html#why-workflow",
    "title": "Workflow",
    "section": "Why Workflow?",
    "text": "Why Workflow?\nBlueprint’s data analysts have built tools and practices that suit their various needs. As a collective, we do data analysis well in the status quo model. Why establish standards for organizing and executing data analysis projects? Because we think that the whole process of data analysis could be easier to do, easier to learn, more predictable for managers, more collaborative for data goblins, and make more contributions to our collective intelligence as a community of practice."
  },
  {
    "objectID": "wf.html#recommended-reading",
    "href": "wf.html#recommended-reading",
    "title": "Workflow",
    "section": "Recommended Reading",
    "text": "Recommended Reading\nThroughout this section, we make reference to a number of resources created by the R development and data analysis community. Each is a worthwhile read in its own right, and we recommend that you check them out at some point.\n\nBryan, J. Happy Git and GitHub for the useR\nThe ur-text for R users looking to integrate git and Github into their analysis workflows. Bryan keeps it fun and friendly while providing a thorough introduction to the nuts and bolts of getting started. Closer to required than recommended reading.\n\n\nWickham, H. R for Data Science\nHadley is the GOAT R developer, and this book – while a bit sparse and introductory – offers valuable advice and instruction across the whole process, with some starting points for analysis workflow.\n\n\nGit Commands Cheatsheet\nGit can do many different things, and so comes with many different commands. This is a fairly comprehensive reference. Use it if you know what you want to do, you know git can do it, but you’re just not sure which command will make it happen."
  },
  {
    "objectID": "wf-git.html#why-git-and-github",
    "href": "wf-git.html#why-git-and-github",
    "title": "2  Git and Github",
    "section": "2.1 Why Git and Github?",
    "text": "2.1 Why Git and Github?\nIn 2017, Jennifer Bryan expanded on the ‘Why Git?’ chapter of her invaluable instructional book in the paper Excuse me, do you have a moment to talk about version control?. She articulates the benefits so clearly that they are not worth rephrasing:\n\nDoing your work becomes tightly integrated with organizing, recording, and disseminating it. It’s not a separate, burdensome task you are tempted to neglect.\nCollaboration is much more structured, with powerful tools for asynchronous work and managing versions.\nThe marginal effort required to create a web presence for a project is negligible.\nBy using common mechanics across work modes (research, teaching, analysis), you achieve basic competence quickly and avoid the demoralizing forget-relearn cycle.\n\nIf you have worked on a data analysis project at Blueprint, particularly one that involves collaboration, you have undoubtedly done the following:\n\nSearching for an explanation about when or why a specific choice about processing the data was made\nExperienced disorientation attempting to navigate someone else’s code\nSpent hours undoing a change that breaks the pipeline\nWanted someone’s help or advice about how to solve a problem, but didn’t feel like it would be worth getting them access\n\nHosted version control like Github empowers you with tools to simplify and routinize any of these situations."
  },
  {
    "objectID": "wf-git.html#gitting-rolling",
    "href": "wf-git.html#gitting-rolling",
    "title": "2  Git and Github",
    "section": "2.2 Gitting rolling",
    "text": "2.2 Gitting rolling\nAt a minimum, you’ll need to install git and register a Github account. Optionally, you can also integrate git and Github into your RStudio environment. If you’ve done those things already, great, you’re good to go! Otherwise, it is strongly recommended that you pause what you’re doing now and read / work through Happy Git before continuing."
  },
  {
    "objectID": "wf-git.html#usage-at-blueprint-as-of-may-2023",
    "href": "wf-git.html#usage-at-blueprint-as-of-may-2023",
    "title": "2  Git and Github",
    "section": "2.3 Usage at Blueprint (as of May 2023)",
    "text": "2.3 Usage at Blueprint (as of May 2023)\nRight now, usage is highly varied. Some members of the Data Lab team use git and Github for version control and hosting of both R packages and analysis project code. It is not, however, part of the typical analysis project. We will begin rolling out the basic workflow in the summer of 2023 and see how it goes."
  },
  {
    "objectID": "wf-structure.html#a-conceptual-skeleton",
    "href": "wf-structure.html#a-conceptual-skeleton",
    "title": "3  Folder Structure",
    "section": "3.1 A Conceptual Skeleton",
    "text": "3.1 A Conceptual Skeleton\nThe diagram below presents a basic revision of the aforementioned structure. The two substantial differences are:\n\nrather than living within the same folder, data and code live in different places. Data stay in the project’s Z drive folder; code, outputs, figures in the analysis repo (folder).\nthe project’s analysis repo is versioned with git and reflected in a remote repo hosted on GitHub\n\n\n\n\n\nflowchart LR\n    P(Project Library)\n    MP(My Project)\n    D[Intranet folder including \\nDeliverables, Admin docs, etc.]\n    P --- MP\n    MP --- D\n    Z(Z-drive)\n    MZ(My Project)\n    S[Encrypted data folder including\\nSurvey Exports, Program Data, etc.]\n    Z --- MZ\n    MZ --- S\n    L(my-project)\n    ML(My Laptop)\n    C[local git repository, containing\\nCode, reports, figures]\n    ML --- L\n    L --- C\n    GH(Github)\n    R(my-username/my-project)\n    CR[remote git repository\\nreflecting its local counterpart]\n    GH --- R\n    R --- CR\n\n\n\n\n\nThis concept intentionally leaves the substructure of the data folder Z:/My Project and the analysis repo My Laptop/my-project up to the judgement and preference of the project’s specific team. The key difference from the typical current structure is strictly that data (and only data) live in the Z-drive.\n\n\n\n\n\n\nGuidance and tools for setting up analysis repos according to your needs is forthcoming, but it will be in a different section."
  },
  {
    "objectID": "wf-structure.html#what-else-is-new",
    "href": "wf-structure.html#what-else-is-new",
    "title": "3  Folder Structure",
    "section": "3.2 What else is new?",
    "text": "3.2 What else is new?\nLocating analysis code and outputs outside of the Z-drive entails some considerations and requirements that were previously irrelevant:\n\nRemote analysis repositories (GitHub) should be set to private before any outputs are generated. Any colleagues who need to see the code should be invited to collaborate.\nPaths to data files used in code will need to be “hard-coded”, meaning that they will need to begin with “Z:/My Project/”. We’re working on ways to make this easier and more flexible, but hard-coding (in this case) will definitely work.\nBecause files and folders in Z:/My Project will be referred to directly, the names of and paths to files and folders in Z:/My Project should, unless absolutely necessary, be left unchanged from the time that they are created.\nEnsure that outputs (reports, figures, tables) don’t include any personal identifying information. While they won’t be available to the general public in a private Github repo, our policy is to confine such sensitive information to our Z-drive."
  },
  {
    "objectID": "wf-basic.html#creating-an-analysis-repo",
    "href": "wf-basic.html#creating-an-analysis-repo",
    "title": "4  Basic Workflow",
    "section": "4.1 Creating an Analysis Repo",
    "text": "4.1 Creating an Analysis Repo\nAn analysis repo is basically just an R project. You can set one up through the RStudio GUI, or with the usethis package. usethis also includes handy helpers for initializing a repository, so I tend to rely on it for each step.\nusethis::create_project(\"my-project\")\nThen, open the project folder in your development environment (RStudio or VSCode), and run the following in your R console1.\nusethis::use_git()\nusethis::use_github(private = TRUE)\nAssuming you have set up your Github credentials, your analysis repo will be up and running locally and remotely."
  },
  {
    "objectID": "wf-basic.html#narrowing-the-scope",
    "href": "wf-basic.html#narrowing-the-scope",
    "title": "4  Basic Workflow",
    "section": "4.2 Narrowing the scope",
    "text": "4.2 Narrowing the scope\nAs mentioned previously, git can do a lot. For this basic workflow, you’re only going to use three commands to make it run: git add git commit and git push. We assume you’re the only one working on the analysis repo. If you are collaborating with others, and especially if you are working on different aspects of the project at the same time, please reach out for more guidance if you’re feeling unsure."
  },
  {
    "objectID": "wf-basic.html#the-basic-pattern",
    "href": "wf-basic.html#the-basic-pattern",
    "title": "4  Basic Workflow",
    "section": "4.3 The Basic Pattern",
    "text": "4.3 The Basic Pattern\n\nIdentify a task (like “link survey data”, “clean program records”, “explore demographics”)\nWrite the code that completes the task (you’ve been here before)\ngit add -A to stage your changes to the analysis repo\ngit commit -m \"link survey data\" 2 to commit your changes with the message “link survey data” (use whatever task you’re actually doing)\ngit push -u origin main to upload your local changes to the remote (github) repository.\n\nThis basic pattern is a simplified version of the Repeated Amend workflow pattern described in Happy Git. You’re welcome to use that pattern as well, or explore Branching and Merging."
  },
  {
    "objectID": "wf-basic.html#what-can-i-do-now-that-i-couldnt-before",
    "href": "wf-basic.html#what-can-i-do-now-that-i-couldnt-before",
    "title": "4  Basic Workflow",
    "section": "4.4 What can I do now that I couldn’t before?",
    "text": "4.4 What can I do now that I couldn’t before?\n\nAsk for help: if you’ve run into a problem that you don’t feel equipped to solve, you can ask one of your colleagues (with access to the Z-drive data folder) to clone the analysis repo, run your code, and offer edits, advice, or whatever else would help.\nShow your work: the analysis repo is less sensitive than the data folder, and so can be shown to any Blueprinter, whether they are on the project team or not.\nFind solutions: get inspiration from other data analyzing printers using the same approach by borrowing their code."
  },
  {
    "objectID": "wf-basic.html#footnotes",
    "href": "wf-basic.html#footnotes",
    "title": "4  Basic Workflow",
    "section": "",
    "text": "The R console, where you execute R commands, is not the same as the Terminal (a.k.a. Shell, Command Prompt) You may not have used the terminal before, but you should be able to access it within your RStudio interface. See again Happy Git for more detail.↩︎\nDo your best to include a commit message every time you make a commit. Git requires that every commit have an accompanying message, and if you don’t provide in the shell directly, it will send you to vim, a much more confusing program.↩︎"
  },
  {
    "objectID": "wf-goal.html",
    "href": "wf-goal.html",
    "title": "5  Advanced / Aspirational Workflow",
    "section": "",
    "text": "This section is still quite rough. If you’re here, you’re welcome to drop suggested changes, additions, or clarifications in the comments. Your input will be incorporated into the development process.\n\n\n\nBolded items are clear hooks for tools, rules, standards.\nAP[X] is the Xth analysis planning stage R[X] is the Xth artefact of the analysis process\n\nPre-contract\n\n\nProposal\nResearch objectives\nAP0: Preliminary plan\n\nR0: High-level methodology\n\n\n\nData design\n\n\nSurvey designs\nAdmin datasets\nAP1: Specify approach\n\nR1: Detailed methodology\nR2: Input data dictionary\n\n\n\nData collection\n\n\nMonitoring:\n\nR3: Monitoring log\n\n\n\nAnalyze\n\n\nHave the data for the analysis\nHave accumulated insights\nHave monitoring logs\nExplore loop:\n\nR4: Exploratory output pile\nR5: Decision log\n\nAP2: revisit R1: Detailed methodology\n\nRealign analysis objectives with project objectives\nAssess feasibility through exploration\nIdentify limitations and enumerate implications\nJustify methodological adapatations\nIteratively build R6: data pipeline\nDocument final data pipeline\nFinalize methodology\n\n\n\nPublish\n\n\nAP3: Schedule of deliverable outputs\n\nR7: Analysis outputs"
  },
  {
    "objectID": "viz.html#recomended-reading",
    "href": "viz.html#recomended-reading",
    "title": "Visualization",
    "section": "Recomended Reading",
    "text": "Recomended Reading\nMost of the recommended readings in this section focus on the conceptual approach to tackling visualization problems. This is intentional, since designing the right visualization is by far the deeper and more complex challenge than instructing a computer how to make it.\n\nAndrew Heiss, Data Visualization\nA thorough yet accessible free course on data visualization in general, illustrated with practical applications in R and ggplot.\n\n\nEdward Tufte, The Visual Display of Quantitative Information\nThe way that information is visualized in academia, business, and the media makes Edward Tufte sick. In this book, he explains what makes him mad, why it infuriates him so, and how it could be different. While his puritanical minimalism often overshoots its target (occasionally at the expense of clarity), it will transform the way you think about designing charts, tables, and maps for the better.\n\n\nAlberto Cairo, The Functional Art\nCairo comes from a similar philosophical perspective as Tufte, but is in turns more pragmatic, patient, and permissive. Instead of characterizing any non-informative or redundant element of a visualization as ‘chart-junk’, he develops a useful concept of the different objectives visualizations can be designed to achieve, and discussses the tradeoff between depth and familiarity with a good deal more nuance.\n\n\nHadley Wickham et al, ggplot2\nAn in-depth guide to ggplot2, the standard software package for visualization at Blueprint."
  },
  {
    "objectID": "viz-why.html#exploration",
    "href": "viz-why.html#exploration",
    "title": "6  Why Visualize?",
    "section": "6.1 Exploration",
    "text": "6.1 Exploration\n\ntesting your prior expectations about the data\nidentifying patterns and relationships\nvisually confirming the results of processing"
  },
  {
    "objectID": "viz-why.html#presentation",
    "href": "viz-why.html#presentation",
    "title": "6  Why Visualize?",
    "section": "6.2 Presentation",
    "text": "6.2 Presentation\n\nsupporting an argument\nanchoring an explanation\nimpactfully illustrating a comparison"
  },
  {
    "objectID": "viz-how.html#for-exploration",
    "href": "viz-how.html#for-exploration",
    "title": "7  How to Visualize",
    "section": "7.1 For exploration",
    "text": "7.1 For exploration"
  },
  {
    "objectID": "viz-how.html#for-presentation",
    "href": "viz-how.html#for-presentation",
    "title": "7  How to Visualize",
    "section": "7.2 For presentation",
    "text": "7.2 For presentation"
  },
  {
    "objectID": "viz-how.html#ggplot2",
    "href": "viz-how.html#ggplot2",
    "title": "7  How to Visualize",
    "section": "7.3 ggplot2",
    "text": "7.3 ggplot2"
  },
  {
    "objectID": "viz-how.html#other-software",
    "href": "viz-how.html#other-software",
    "title": "7  How to Visualize",
    "section": "7.4 Other software",
    "text": "7.4 Other software"
  },
  {
    "objectID": "sd.html",
    "href": "sd.html",
    "title": "Developing Your Skills",
    "section": "",
    "text": "This chapter is currently under active development. Use its content as it is helpful, but please moderate your expectations. If you’re interested in helping out with its development, head over to the repo\n\n\n\nThe Blueprint Data Lab’s mandate is not merely to ensure that our firm remains on the cutting edge of data-driven solutions, but also to foster an environment of growth, learning, and curiosity. The world of data is continuously evolving, with new techniques, technologies, and paradigms emerging regularly. As such, continuous learning across the organization is not only beneficial but critical for us to remain adept and agile in navigating the complex challenges we find ourselves faced with every day.\nTo thrive in our work, we must harness the , using it to shape insights and drive decision-making. Developing our skills in four main areas—programming, statistics, analysis, and visualization—is thus integral to our work as consultants. These are the pillars of data literacy that form the foundation of our professional expertise.\n\nProgramming: In an age where automation is king, knowing how to code is essential. Programming allows us to extract, clean, process, and manipulate data with precision and speed. It opens up vast possibilities for data exploration and analysis that manual methods simply cannot match.\nStatistics: This is the language of data. It provides the mathematical framework for understanding, interpreting, and making inferences from data. A solid grounding in statistics is necessary for us to distill sense from the noisy world of raw data and to validate the insights we generate.\nAnalysis: This skill involves putting the tools of programming and the principles of statistics to work together. It’s about identifying patterns and trends, solving problems, and making meaning. It’s an investigative process that demands both rigorous scientific thinking and creative intuition.\nVisualization: It’s one thing to find insights in data; it’s another to communicate them effectively. Data visualization is about using graphical representations to tell the story of the data in a way that is intuitive and compelling. It bridges the gap between data experts and non-experts, making our insights accessible to all.\nEquity: In alignment with Blueprint’s core values, we must do our best to ensure that our data analysis forwards the goals of a more equitable and just society. This means more than just identifying and mitigating the analytical risks posed by structural, societal, and methodological biases. It means situating any data analysis we carry out within an anti-oppression (anti-racist, anti-colonialist, anti-ablist) philosophical framework.\n\nIn this chapter, “Developing Your Skills,” you’ll find a curated list of resources designed to help you strengthen these four pillars. They range from introductory courses for those new to the field, to advanced readings for seasoned professionals seeking to deepen their knowledge. Each resource has been carefully selected based on its educational value, practical relevance, and alignment with our firm’s work and values.\nRemember, as professionals in a field that’s ever-changing, our learning journey never ends. It’s my hope that this resource guide will inspire you to continuously cultivate your skills, innovate in your work, and strive towards mastery. With the right tools, the right mindset, and a dedication to lifelong learning, there’s no limit to the impact we can achieve.\nWelcome to this exciting chapter of your professional development."
  },
  {
    "objectID": "sd-courses.html",
    "href": "sd-courses.html",
    "title": "9  Courses",
    "section": "",
    "text": "Programming Languages\nNand2Tetris\nAlgorithms\nStatistical Rethinking\nBayesian Statistics\nComputational Methods for Social Science Data\nMicroeconomics for Public Policy"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Knuth, Donald E. 1984. “Literate Programming.” Comput.\nJ. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  }
]